import os
import json
import requests
from datetime import datetime
from dotenv import load_dotenv
from google import genai
import sys

# 1. í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()
client = genai.Client(api_key=os.getenv("GOOGLE_API_KEY"))
TELEGRAM_TOKEN = os.getenv("TELEGRAM_TOKEN")
CHAT_ID = os.getenv("MY_PRIVATE_CHAT_ID")

DATA_FILE = "ai_research_data.json"
HISTORY_FILE = "ai_research_history.json"

def send_telegram(message):
    """í…”ë ˆê·¸ë¨ ë©”ì‹œì§€ ì „ì†¡"""
    url = f"https://api.telegram.org/bot{TELEGRAM_TOKEN}/sendMessage"
    if len(message) > 4000: message = message[:3900] + "\n\n...(ì´í•˜ ìƒëµ)"
    try:
        requests.post(url, json={"chat_id": CHAT_ID, "text": message, "parse_mode": "Markdown"}, timeout=10)
    except Exception as e:
        print(f"Telegram Error: {e}")

def get_latest_research():
    """ìµœì‹  AI Scientist/Virtual Lab ë…¼ë¬¸ ê²€ìƒ‰ ê°€ìƒ ë¡œì§ (ì‹¤ì œëŠ” API/RSS ì—°ë™)"""
    # ë°ëª¨ìš© ìƒ˜í”Œ ë°ì´í„°
    return [
        {
            "title": "Autonomous Hypothesis Generation via Multi-Agent Consensus",
            "journal": "bioRxiv",
            "date": "2026-02",
            "url": "https://biorxiv.org/example/1",
            "summary": "AI agents reaching consensus on biological hypotheses before experimental design."
        }
    ]

def analyze_research_with_ai(research_list):
    """Geminië¥¼ ì‚¬ìš©í•˜ì—¬ ì—°êµ¬ ë™í–¥ ë¶„ì„ ë° PPTìš© ë°ì´í„° ê°€ê³µ"""
    prompt = f"""
    ë‹¹ì‹ ì€ ì¸ê³µì§€ëŠ¥ ë° ë°”ì´ì˜¤ ê¸°ìˆ  ì „ëµ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì•„ë˜ ìµœì‹  ì—°êµ¬ ë¦¬ìŠ¤íŠ¸ë¥¼ ë¶„ì„í•˜ì—¬ 
    ì „ëµ ë³´ê³ ìš© 'PPT ìŠ¬ë¼ì´ë“œ êµ¬ì¡°'ì™€ 'NotebookLMìš© ì†ŒìŠ¤'ë¥¼ ì‘ì„±í•˜ì‹­ì‹œì˜¤.
    
    [ì—°êµ¬ ë¦¬ìŠ¤íŠ¸]
    {research_list}
    
    [ë¶„ì„ ì§€ì¹¨]
    1. 'Virtual Lab of AI agents' ê´€ì ì—ì„œ ì—°êµ¬ì˜ í˜ì‹ ì„±ì„ í‰ê°€í•˜ì‹­ì‹œì˜¤.
    2. PPT ìŠ¬ë¼ì´ë“œ 5ì¥ ë¶„ëŸ‰ì˜ êµ¬ì„±ì•ˆì„ ì‘ì„±í•˜ì‹­ì‹œì˜¤ (ì œëª©/í•µì‹¬ê³µí—Œ/ë°©ë²•ë¡ /ì„±ê³¼/ì‹œì‚¬ì ).
    3. ì¸í¬ê·¸ë˜í”½ìœ¼ë¡œ ì‹œê°í™”í•˜ê¸° ì¢‹ì€ ì§€í‘œë‚˜ ê´€ê³„ë„ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë¬˜ì‚¬í•˜ì‹­ì‹œì˜¤.
    
    ê²°ê³¼ëŠ” í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ì‹­ì‹œì˜¤.
    """
    try:
        response = client.models.generate_content(model='models/gemini-2.0-flash', contents=prompt)
        return response.text.strip()
    except Exception as e:
        return f"AI ë¶„ì„ ì˜¤ë¥˜: {e}"

def monitor_cycle():
    """ì£¼ê°„ ëª¨ë‹ˆí„°ë§ ì‹¤í–‰"""
    new_researches = get_latest_research()
    
    if os.path.exists(HISTORY_FILE):
        with open(HISTORY_FILE, 'r', encoding='utf-8') as f:
            history = json.load(f)
    else:
        history = []

    analysis_report = analyze_research_with_ai(new_researches)
    
    # í…”ë ˆê·¸ë¨ ì „ì†¡
    report_msg = f"ğŸ”¬ *[ì£¼ê°„ AI ê°€ìƒ ì—°êµ¬ì†Œ ë™í–¥]*\n\n{analysis_report[:1000]}..."
    send_telegram(report_msg)
    
    # íˆìŠ¤í† ë¦¬ ì €ì¥
    history.append({
        "date": datetime.now().strftime("%Y-%m-%d"),
        "researches": new_researches,
        "analysis": analysis_report
    })
    
    with open(HISTORY_FILE, 'w', encoding='utf-8') as f:
        json.dump(history, f, indent=4, ensure_ascii=False)
    
    # NotebookLMìš© ì†ŒìŠ¤ íŒŒì¼ ì—…ë°ì´íŠ¸
    with open("notebook_expert_source.txt", "w", encoding="utf-8") as f:
        f.write(analysis_report)
        
    print("AI Research Monitor updated.")

if __name__ == "__main__":
    monitor_cycle()
